# ğŸ› ï¸ Data Engineering Portfolio by Marcio Najarro

![CI](https://github.com/Managall/data-engineering-portfolio/actions/workflows/python-tests.yml/badge.svg)

Bienvenido a mi portafolio de proyectos de IngenierÃ­a de Datos. AquÃ­ encontrarÃ¡s soluciones prÃ¡cticas diseÃ±adas para construir experiencia real en ETL, automatizaciÃ³n, orquestaciÃ³n, pipelines en la nube y mÃ¡s.

---

## ğŸ“ Proyecto 1: ETL BÃ¡sico con Python y SQLite

Este proyecto construye un pipeline ETL con enfoque modular y buenas prÃ¡cticas de desarrollo. Incluye:

- ExtracciÃ³n de datos desde un archivo CSV.
- Transformaciones bÃ¡sicas con pandas.
- Carga a base de datos SQLite.
- Logging estructurado.
- Tests automatizados con pytest.
- IntegraciÃ³n continua con GitHub Actions.

---

### ğŸ§° TecnologÃ­as usadas

- Python 3.11
- pandas
- sqlite3
- pytest
- GitHub Actions
- Logging con `logging`
- CI/CD con workflows `.yml`
- Estructura de proyecto escalable y profesional

---

### ğŸ“‚ Estructura

```
etl-basic-project/
â”œâ”€â”€ etl/                    # MÃ³dulos ETL
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ tests/                 # Pruebas unitarias
â”œâ”€â”€ data/                  # CSV de entrada y SQLite DB
â”œâ”€â”€ config.py              # Rutas y variables globales
â”œâ”€â”€ main.py                # Pipeline ejecutable
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

### âš™ï¸ CÃ³mo ejecutar el pipeline

```bash
# Activar entorno virtual
source venv/bin/activate       # Linux/macOS
.env\Scriptsctivate        # Windows

# Instalar dependencias
pip install -r etl-basic-project/requirements.txt

# Ejecutar el pipeline
cd etl-basic-project
python main.py
```

---

### ğŸ§ª CÃ³mo ejecutar las pruebas

```bash
cd etl-basic-project
pytest
```

Las pruebas tambiÃ©n se ejecutan automÃ¡ticamente en GitHub Actions al hacer push.

---

### ğŸ“ˆ PrÃ³ximos proyectos

- **Proyecto 2:** AutomatizaciÃ³n del pipeline ETL con `schedule`, `cron` o `APScheduler`.
- **Proyecto 3:** Pipeline en la nube con AWS S3, Glue y Redshift.
- **Proyecto 4:** Procesamiento en streaming con Kafka o Kinesis.
- **Proyecto 5:** Lakehouse con arquitectura Medallion en Databricks o PySpark.

---

**Marcio Najarro**  
Data Engineer en formaciÃ³n  
ğŸ“« marcionajarro@gmail.com | [LinkedIn](www.linkedin.com/in/marcio-andrea-najarro-gallegos-4018b7140)
